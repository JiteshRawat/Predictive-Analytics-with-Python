{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, scale\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, log_loss, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, precision_score, fbeta_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "\n",
    "seed = 999\n",
    "\n",
    "creditcard = pd.read_csv('./data/creditcard.csv')\n",
    "creditcard.columns = [x.lower() for x in creditcard.columns]\n",
    "creditcard.rename(columns = {'class': 'fraud'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split Test Data Out\n",
    "creditcard.drop(columns = 'time', inplace = True)\n",
    "\n",
    "# Normalize the 'amount' column\n",
    "scaler = StandardScaler()\n",
    "creditcard['amount'] = scaler.fit_transform(creditcard['amount'].values.reshape(-1, 1))\n",
    "# creditcard.drop(columns = 'amount', inplace = True)\n",
    "\n",
    "X = creditcard.iloc[:, :-1]\n",
    "y = creditcard.iloc[:, -1]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually for imbalanced data, we can try:\n",
    "\n",
    "1. Collect more data (which not work here since the data is given)\n",
    "2. Down-Sampling or Over-Sampling to get balanced samples\n",
    "3. Change the Thresholds to adjust the prediction\n",
    "4. Assign class weights for the low rate class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use the Imbalanced Data Directly in RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creditcard.iloc[:, :-1]\n",
    "y = creditcard.iloc[:, -1]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)\n",
    "\n",
    "estimator = RandomForestClassifier(random_state=0, warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_parameters = {\"max_depth\": [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 555, in run\n",
      "    result_item, is_broken, bpe = self.wait_result_broken_or_wakeup()\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 609, in wait_result_broken_or_wakeup\n",
      "    ready = wait(readers + worker_sentinels)\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\multiprocessing\\connection.py\", line 869, in wait\n",
      "    ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)\n",
      "  File \"C:\\Users\\Owner\\Anaconda3\\lib\\multiprocessing\\connection.py\", line 801, in _exhaustive_wait\n",
      "    res = _winapi.WaitForMultipleObjects(L, False, timeout)\n",
      "ValueError: need at most 63 handles, got a sequence of length 71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score\n",
    "cv_grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cv_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = cv_grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_name in sorted(rf_tuned_parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = cv_grid.predict(Xtest)\n",
    "print(recall_score(ytest, pred_test))     # 0.65\n",
    "print(precision_score(ytest, pred_test))  # 0.85\n",
    "print(roc_auc_score(ytest, pred_test))    # 0.83\n",
    "print(\"confustion matrix on validation data: \\n\" + str(confusion_matrix(ytest, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Over-sampling data and Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1\n",
    "# repeat the positive data for X and y\n",
    "ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)\n",
    "Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)\n",
    "# concat the repeated data with the original data together\n",
    "ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)\n",
    "Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50\n",
    "\n",
    "estimator = RandomForestClassifier(random_state=0, warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_parameters = {\"max_depth\": [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score\n",
    "cv_grid.fit(Xtrain_oversample, ytrain_oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cv_grid.best_params_\n",
    "# print cv_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = cv_grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_name in sorted(rf_tuned_parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = cv_grid.predict(Xtest)\n",
    "print(recall_score(ytest, pred_test))     # 0.83\n",
    "print(precision_score(ytest, pred_test))  # 0.83\n",
    "print(roc_auc_score(ytest, pred_test))    # 0.92\n",
    "print(\"\\n confustion matrix on validation data: \\n\" + str(confusion_matrix(ytest, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RandomForestClassifier with class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = creditcard.iloc[:, :-1]\n",
    "y = creditcard.iloc[:, -1]\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_weight = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1\n",
    "\n",
    "estimator = RandomForestClassifier(random_state=0, class_weight = {0 : 1, 1 : positive_weight}, warm_start = True)\n",
    "\n",
    "rf_tuned_parameters = {\"max_depth\": [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}\n",
    "\n",
    "cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score\n",
    "cv_grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cv_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = cv_grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_name in sorted(rf_tuned_parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = cv_grid.predict(Xtest)\n",
    "print(recall_score(ytest, pred_test))     #  0.85\n",
    "print(precision_score(ytest, pred_test))  #  0.81\n",
    "print(roc_auc_score(ytest, pred_test))    #  0.92\n",
    "print(\"\\n confustion matrix on validation data: \\n\" + str(confusion_matrix(ytest, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-defined Score and GridSearchCV of hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(ground_truth, predictions):\n",
    "    '''\n",
    "    based on results above about the average loss from false positive and false negative predictions.\n",
    "    '''\n",
    "    cmatrix = confusion_matrix(ground_truth, predictions)\n",
    "    fp = cmatrix[0, 1]\n",
    "    fn = cmatrix[1, 0]\n",
    "    return  fn * 122 + fp * 1.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_loss_score = make_scorer(scoring, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1\n",
    "# repeat the positive data for X and y\n",
    "ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)\n",
    "Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)\n",
    "# concat the repeated data with the original data together\n",
    "ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)\n",
    "Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(random_state=0, warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned_parameters = {\"max_depth\": [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], \n",
    "                       'min_samples_leaf': [10, 20, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = wt_loss_score, verbose = 5, n_jobs = 70)\n",
    "cv_grid.fit(Xtrain_oversample, ytrain_oversample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cv_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = cv_grid.predict(Xtest)\n",
    "print(recall_score(ytest, pred_test))     # 0.84\n",
    "print(precision_score(ytest, pred_test))  # 0.84\n",
    "print(roc_auc_score(ytest, pred_test))    # 0.92\n",
    "print(\"\\n confustion matrix on validation data: \\n\" + str(confusion_matrix(ytest, pred_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
